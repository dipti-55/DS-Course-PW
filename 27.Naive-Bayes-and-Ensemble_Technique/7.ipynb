{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Assignment Questions:\n",
    "\n",
    "#### Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "**Design a pipeline that includes the following steps:**\n",
    "\n",
    "1. **Use an automated feature selection method to identify the important features in the dataset**  \n",
    "   - Use `SelectKBest` or `RFE` for feature selection.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.feature_selection import SelectKBest, f_classif\n",
    "   selector = SelectKBest(f_classif, k=10)  # Select top 10 features\n",
    "   X_new = selector.fit_transform(X, y)\n",
    "   ```\n",
    "\n",
    "2. **Create a numerical pipeline that includes the following steps:**\n",
    "   - **Impute the missing values in the numerical columns using the mean of the column values.**\n",
    "   - **Scale the numerical columns using standardization.**\n",
    "\n",
    "   ```python\n",
    "   from sklearn.pipeline import Pipeline\n",
    "   from sklearn.impute import SimpleImputer\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "   numerical_pipeline = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean')),\n",
    "       ('scaler', StandardScaler())\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "3. **Create a categorical pipeline that includes the following steps:**\n",
    "   - **Impute the missing values in the categorical columns using the most frequent value of the column.**\n",
    "   - **One-hot encode the categorical columns.**\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "   categorical_pipeline = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "       ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "4. **Combine the numerical and categorical pipelines using a `ColumnTransformer`.**\n",
    "\n",
    "   ```python\n",
    "   from sklearn.compose import ColumnTransformer\n",
    "\n",
    "   preprocessor = ColumnTransformer(transformers=[\n",
    "       ('num', numerical_pipeline, numerical_features),\n",
    "       ('cat', categorical_pipeline, categorical_features)\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "5. **Use a Random Forest Classifier to build the final model.**\n",
    "\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "   model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier())])\n",
    "   ```\n",
    "\n",
    "6. **Evaluate the accuracy of the model on the test dataset.**\n",
    "\n",
    "   ```python\n",
    "   model.fit(X_train, y_train)\n",
    "   y_pred = model.predict(X_test)\n",
    "\n",
    "   from sklearn.metrics import accuracy_score\n",
    "   accuracy = accuracy_score(y_test, y_pred)\n",
    "   print(f\"Accuracy: {accuracy:.2f}\")\n",
    "   ```\n",
    "\n",
    "**Interpretation & Possible Improvements:**\n",
    "- The pipeline automates feature selection, imputation, and scaling, making it efficient.\n",
    "- Improvements could include experimenting with different models, fine-tuning hyperparameters using `GridSearchCV`, and addressing multicollinearity by removing correlated features.\n",
    "\n",
    "#### Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **Load the Iris dataset**:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.datasets import load_iris\n",
    "   data = load_iris()\n",
    "   X, y = data.data, data.target\n",
    "   ```\n",
    "\n",
    "2. **Build pipelines for Random Forest and Logistic Regression**:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "   from sklearn.pipeline import Pipeline\n",
    "\n",
    "   rf_pipeline = Pipeline([('classifier', RandomForestClassifier())])\n",
    "   lr_pipeline = Pipeline([('classifier', LogisticRegression())])\n",
    "   ```\n",
    "\n",
    "3. **Combine the classifiers using a Voting Classifier**:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "   voting_clf = VotingClassifier(estimators=[\n",
    "       ('rf', RandomForestClassifier()),\n",
    "       ('lr', LogisticRegression())\n",
    "   ], voting='hard')\n",
    "   ```\n",
    "\n",
    "4. **Train and evaluate the pipeline**:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.metrics import accuracy_score\n",
    "\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "   \n",
    "   voting_clf.fit(X_train, y_train)\n",
    "   y_pred = voting_clf.predict(X_test)\n",
    "   \n",
    "   accuracy = accuracy_score(y_test, y_pred)\n",
    "   print(f\"Accuracy: {accuracy:.2f}\")\n",
    "   ```\n",
    "\n",
    "**Interpretation**:\n",
    "- The Voting Classifier combines predictions from both models, resulting in potentially better performance than using either model alone.\n",
    "- **Improvements**: Experiment with different voting schemes (`soft` voting) and more diverse classifiers such as SVM or KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
