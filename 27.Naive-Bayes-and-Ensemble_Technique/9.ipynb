{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "### Q1. What is Gradient Boosting Regression?\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It combines the predictions of several weak learners, typically decision trees, to improve accuracy. In each step, the algorithm fits a new model to the residual errors (the difference between the actual and predicted values) of the previous model, gradually improving the overall performance.\n",
    "\n",
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "To implement Gradient Boosting from scratch, you need to:\n",
    "- Initialize a prediction (e.g., the mean of the target).\n",
    "- Compute the residuals.\n",
    "- Fit a weak learner (a decision tree) on these residuals.\n",
    "- Update the predictions by adding the predictions from the weak learner multiplied by the learning rate.\n",
    "- Repeat for a specified number of iterations.\n",
    "\n",
    "After implementing the model, evaluate it using metrics such as Mean Squared Error (MSE) and R-squared, which quantify the model's performance.\n",
    "\n",
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "In this task, you'll experiment with hyperparameters like:\n",
    "- **Learning rate**: The step size used when updating predictions.\n",
    "- **Number of trees**: The number of weak learners added to the ensemble.\n",
    "- **Tree depth**: Controls how complex the individual decision trees are.\n",
    "You can perform Grid Search or Random Search to find the best combination of these parameters.\n",
    "\n",
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "A weak learner is a model that performs slightly better than random guessing. In Gradient Boosting, decision trees with limited depth (often shallow trees) are typically used as weak learners because they capture simple patterns and contribute small improvements in performance.\n",
    "\n",
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "The intuition behind Gradient Boosting is that you can improve a model by iteratively fitting weak learners to correct the errors made by previous learners. By combining many weak learners, each focusing on the residual errors of the previous learners, the overall model becomes stronger and more accurate.\n",
    "\n",
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Gradient Boosting builds an ensemble by iteratively adding weak learners. Each weak learner is trained on the residuals (errors) from the previous round of predictions. This ensures that each learner focuses on the errors made by the previous models, gradually improving the overall performance.\n",
    "\n",
    "### Q7. What are the steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm?\n",
    "The key mathematical steps are:\n",
    "1. **Initialize the model** with a constant prediction, such as the mean of the target variable.\n",
    "2. **Compute the residuals** (the difference between actual and predicted values).\n",
    "3. **Fit a weak learner** (like a decision tree) on the residuals.\n",
    "4. **Update the prediction** by adding the contribution of the weak learner, scaled by the learning rate.\n",
    "5. Repeat the process for a set number of iterations or until convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
