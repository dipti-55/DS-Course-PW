{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assignment: Clustering-2\n",
    "\n",
    "### Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n",
    "**Hierarchical clustering** is a method of cluster analysis that seeks to build a hierarchy of clusters. It is different from other clustering techniques like **K-Means** in that it does not require the number of clusters to be defined upfront. Instead, hierarchical clustering starts with either:\n",
    "- **Agglomerative**: Each data point as its own cluster and then merges the closest pairs step by step.\n",
    "- **Divisive**: All data points in one cluster and recursively splits it into smaller clusters.\n",
    "\n",
    "### Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n",
    "1. **Agglomerative Hierarchical Clustering**: This is a \"bottom-up\" approach where each data point starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "   \n",
    "2. **Divisive Hierarchical Clustering**: This is a \"top-down\" approach where all data points start in one large cluster, and splits are performed recursively as one moves down the hierarchy.\n",
    "\n",
    "### Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?\n",
    "The distance between two clusters can be determined using several methods:\n",
    "- **Single linkage**: The minimum distance between points in the two clusters.\n",
    "- **Complete linkage**: The maximum distance between points in the two clusters.\n",
    "- **Average linkage**: The average distance between points in the two clusters.\n",
    "- **Centroid linkage**: The distance between the centroids of the two clusters.\n",
    "\n",
    "Common distance metrics used include:\n",
    "- **Euclidean distance**\n",
    "- **Manhattan distance**\n",
    "- **Cosine distance**\n",
    "\n",
    "### Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?\n",
    "To determine the optimal number of clusters, the following methods can be used:\n",
    "- **Dendrogram analysis**: Cutting the dendrogram at a certain level can reveal the optimal number of clusters.\n",
    "- **Elbow method**: Plot the sum of squared distances and look for the \"elbow\" point where the rate of decrease slows down.\n",
    "- **Silhouette score**: A method that measures how similar a point is to its own cluster compared to other clusters.\n",
    "\n",
    "### Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
    "A **dendrogram** is a tree-like diagram that records the sequences of merges or splits in hierarchical clustering. It is useful because:\n",
    "- It shows the hierarchical relationship between clusters.\n",
    "- It can help in determining the optimal number of clusters by cutting the dendrogram at different levels.\n",
    "- It provides insights into how similar clusters are to one another.\n",
    "\n",
    "### Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?\n",
    "Yes, hierarchical clustering can be used for both **numerical** and **categorical data**, but the distance metrics differ:\n",
    "- For **numerical data**, distance metrics such as **Euclidean** or **Manhattan** distance are used.\n",
    "- For **categorical data**, other metrics like **Hamming distance** or **Gowerâ€™s distance** are used to account for categorical values.\n",
    "\n",
    "### Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?\n",
    "Hierarchical clustering can identify outliers or anomalies by examining clusters that contain only a few data points. These small clusters or data points that are merged last in an agglomerative clustering approach may be considered anomalies or outliers as they are significantly different from the rest of the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
