{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: Ensemble Techniques and Its Types - 3\n",
    "\n",
    "---\n",
    "\n",
    "### Q1: What is Random Forest Regressor?\n",
    "\n",
    "A **Random Forest Regressor** is an ensemble learning method that uses multiple decision trees to perform regression tasks. It builds a collection of decision trees during training and outputs the average of their predictions for continuous outcomes. Each tree in the forest is constructed from a random subset of the data, making it more robust and less prone to overfitting compared to a single decision tree.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: How does Random Forest Regressor reduce the risk of overfitting?\n",
    "\n",
    "The **Random Forest Regressor** reduces the risk of overfitting by:\n",
    "- Using **bootstrap sampling** (random sampling with replacement) to create multiple subsets of the data.\n",
    "- Building each tree on a different subset of data, which reduces the variance.\n",
    "- Averaging the predictions of all trees, which smooths out the errors from individual overfitted trees, resulting in a more generalized model.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "\n",
    "The **Random Forest Regressor** aggregates the predictions of multiple decision trees by taking the **average** of the predictions from all the individual trees in the forest. Each tree gives a prediction, and the final output is the mean of these predictions, which helps to reduce noise and increase accuracy in regression tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: What are the hyperparameters of Random Forest Regressor?\n",
    "\n",
    "Key **hyperparameters** of the Random Forest Regressor include:\n",
    "- `n_estimators`: The number of trees in the forest.\n",
    "- `max_depth`: The maximum depth of each tree.\n",
    "- `min_samples_split`: The minimum number of samples required to split a node.\n",
    "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n",
    "- `max_features`: The number of features to consider when looking for the best split.\n",
    "- `bootstrap`: Whether bootstrap samples are used when building trees.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "\n",
    "- **Random Forest Regressor**:\n",
    "  - An ensemble of multiple decision trees.\n",
    "  - Reduces overfitting by averaging the predictions of multiple trees.\n",
    "  - Uses random subsets of data and features for training each tree.\n",
    "  \n",
    "- **Decision Tree Regressor**:\n",
    "  - A single tree-based model.\n",
    "  - More prone to overfitting as it attempts to capture all details of the training data.\n",
    "  - No random sampling of data or features.\n",
    "\n",
    "**Key Difference**: Random Forest reduces variance and overfitting by combining the predictions of multiple trees, while Decision Tree Regressor uses only one tree and can be more sensitive to noisy data.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6: What are the advantages and disadvantages of Random Forest Regressor?\n",
    "\n",
    "**Advantages**:\n",
    "- **Reduces overfitting**: Combines predictions from multiple trees, resulting in more accurate and generalized predictions.\n",
    "- **Handles large datasets**: Works well with large datasets and high-dimensional spaces.\n",
    "- **Robust to noise**: Less sensitive to overfitting due to its averaging mechanism.\n",
    "- **Feature importance**: Provides insights into feature importance.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Computationally expensive**: Requires more time and resources to train compared to a single decision tree.\n",
    "- **Less interpretable**: The model is harder to interpret compared to a single decision tree.\n",
    "\n",
    "---\n",
    "\n",
    "### Q7: What is the output of Random Forest Regressor?\n",
    "\n",
    "The output of a **Random Forest Regressor** is a **continuous value**. It takes the average of the predictions made by the individual decision trees in the forest to provide a final regression result.\n",
    "\n",
    "---\n",
    "\n",
    "### Q8: Can Random Forest Regressor be used for classification tasks?\n",
    "\n",
    "Yes, **Random Forest Regressor** can be adapted for classification tasks by using **Random Forest Classifier**, which works similarly but instead of averaging predictions, it takes the majority vote of the class predictions from individual trees. This version is used for tasks where the output is categorical rather than continuous.\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
