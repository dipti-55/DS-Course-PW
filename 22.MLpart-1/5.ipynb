{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "**Answer:**\n",
    "**Min-Max Scaling** is a feature scaling technique that transforms the features to a specific range, typically between 0 and 1, or any other defined range. Itâ€™s used in data preprocessing to normalize the features, ensuring that they have the same scale without distorting differences in the range of values.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\\[\n",
    "X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "\\]\n",
    "Where:\n",
    "- \\(X\\) is the original value,\n",
    "- \\(X_{min}\\) and \\(X_{max}\\) are the minimum and maximum values of the feature.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1], [5], [10], [15], [20]])\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Q2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "**Answer:**\n",
    "The **Unit Vector technique** scales features by normalizing each data point to have a unit norm (length of 1). It is useful when you want to ensure that the magnitude of each feature is consistent, making them comparable in size.\n",
    "\n",
    "The formula is:\n",
    "\\[\n",
    "X_{scaled} = \\frac{X}{\\|X\\|}\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\|X\\| \\) is the Euclidean norm (length) of the vector.\n",
    "\n",
    "**Difference from Min-Max Scaling:**\n",
    "- Min-Max scaling normalizes values between a specific range (e.g., 0 and 1).\n",
    "- Unit vector scaling transforms each data point to have a unit length (1).\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from sklearn.preprocessing import normalize\n",
    "data = np.array([[4, 1], [2, 2], [1, 3]])\n",
    "normalized_data = normalize(data, norm='l2')\n",
    "print(normalized_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Q3: What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "**Answer:**\n",
    "**Principal Component Analysis (PCA)** is a statistical technique used to reduce the dimensionality of datasets by transforming the original features into a set of new, uncorrelated features called **principal components**. These components capture the most significant variance in the data.\n",
    "\n",
    "PCA works by:\n",
    "1. Standardizing the data.\n",
    "2. Calculating the covariance matrix.\n",
    "3. Finding the eigenvectors (principal components) and eigenvalues (variance explained by each component).\n",
    "4. Selecting the top components that capture most of the variance.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "print(pca_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Q4: What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "**Answer:**\n",
    "**Relationship between PCA and Feature Extraction:**\n",
    "PCA can be viewed as a feature extraction technique because it transforms the original features into a new set of features (principal components) that capture the most important information (variance) in the data. These new features are linear combinations of the original ones but have reduced dimensionality and are uncorrelated.\n",
    "\n",
    "**How PCA is used for Feature Extraction:**\n",
    "- PCA projects the data onto a lower-dimensional subspace by selecting a subset of the principal components.\n",
    "- These components are new features that capture the maximum variance in the data.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Assume data has 5 original features, but we extract 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "print(pca_data)  # Reduced dataset with 2 extracted features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Q5: You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "**Answer:**\n",
    "In the recommendation system project, Min-Max scaling can be used to normalize features like price, rating, and delivery time so that they are on the same scale. This helps the model treat these features equally during training, preventing features with larger ranges (e.g., price) from dominating others (e.g., rating).\n",
    "\n",
    "**Steps:**\n",
    "1. **Apply Min-Max scaling** to normalize all features to a range of 0 to 1 (or any desired range).\n",
    "2. **Fit the model** using the scaled data to ensure uniformity across all features.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Preprocessing the dataset with price, rating, and delivery time features\n",
    "data = np.array([[100, 4.5, 30], [200, 4.0, 25], [150, 4.2, 35]])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Q6: You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "**Answer:**\n",
    "For predicting stock prices, PCA can be used to reduce the dimensionality of the dataset by selecting the most important principal components that capture the majority of the variance in the data, simplifying the model and improving its performance.\n",
    "\n",
    "**Steps:**\n",
    "1. **Standardize the data** to ensure each feature has zero mean and unit variance.\n",
    "2. **Apply PCA** to reduce the number of features by selecting the top principal components that capture the most variance.\n",
    "3. **Train the model** using the reduced set of features, improving training time and avoiding overfitting.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "pca = PCA(n_components=5)  # Retain 5 principal components\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "print(pca_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Q7: For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "**Answer:**\n",
    "We can apply Min-Max scaling to transform the given values into the range [-1, 1]. The formula is:\n",
    "\\[\n",
    "X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}} \\times (new\\_max - new\\_min) + new\\_min\n",
    "\\]\n",
    "Where:\n",
    "- \\(new\\_min = -1\\)\n",
    "- \\(new\\_max = 1\\)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "data = np.array([[1], [5], [10], [15], [20]])\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Q8: For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "**Answer:**\n",
    "When performing PCA on features like height, weight, age, gender, and blood pressure, the number of principal components to retain depends on the explained variance ratio. Typically, we choose the number of components that capture at least 95% of the variance to ensure that the most important information in the data is retained.\n",
    "\n",
    "**Steps:**\n",
    "1. **Standardize the features** (except for gender, which is categorical and should be encoded).\n",
    "2. **Apply PCA** and analyze the explained variance ratio.\n",
    "3. **Retain components** that capture 95% or more of the variance.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "pca = PCA(n_components=0.95)  # Retain components that explain 95% variance\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "print(pca_data)\n",
    "```\n",
    "\n",
    "The number of components depends on the cumulative explained variance. Typically, you would retain 2-3 components if they capture sufficient variance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
