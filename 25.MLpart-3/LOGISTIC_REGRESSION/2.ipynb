{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Logistic Regression-2\n",
    "\n",
    "### Q1. What is the purpose of grid search CV in machine learning, and how does it work?\n",
    "\n",
    "Grid Search Cross-Validation (CV) is used to systematically find the best hyperparameters for a machine learning model. The goal is to enhance the modelâ€™s performance by tuning its hyperparameters.\n",
    "\n",
    "**How it works**:\n",
    "1. **Hyperparameter Grid**: You define a grid of possible hyperparameter values.\n",
    "2. **Model Training**: For each combination of hyperparameters, the model is trained.\n",
    "3. **Cross-Validation**: The model performance is evaluated using cross-validation.\n",
    "4. **Best Parameters**: The hyperparameter combination with the best performance metric is selected.\n",
    "\n",
    "This process ensures the model is optimally tuned, leading to better generalization on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. Describe the difference between grid search CV and randomized search CV, and when might you choose one over the other?\n",
    "\n",
    "- **Grid Search CV**: Evaluates every possible combination of hyperparameters from a predefined grid. It is **exhaustive** but can be time-consuming, especially with a large number of parameters.\n",
    "  \n",
    "- **Randomized Search CV**: Evaluates a fixed number of random hyperparameter combinations. It is **faster** and more efficient when the hyperparameter space is large.\n",
    "\n",
    "**When to choose**:\n",
    "- Use **Grid Search** when the parameter space is small, and you can afford to exhaustively search it.\n",
    "- Use **Randomized Search** when the parameter space is large, or you want to perform quicker searches for hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "**Data leakage** occurs when information from outside the training dataset is used to build the model, leading to overly optimistic performance metrics. This is a problem because it causes the model to perform well on training/validation data but fail on unseen data (overfitting).\n",
    "\n",
    "**Example**:\n",
    "If you include future data in training (e.g., the target variable or related information), your model may predict the target too accurately on training data but fail on new data.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "You can prevent data leakage by:\n",
    "- **Proper Data Splitting**: Ensure training, validation, and test sets are separate, and that test data is not used during training.\n",
    "- **Feature Engineering Post-Splitting**: Perform operations like scaling, encoding, and imputing after splitting the data.\n",
    "- **Avoid Using Future Data**: Ensure no future or target-related data is included during training.\n",
    "- **Careful Cross-Validation**: Ensure that cross-validation folds are independent and do not introduce leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "A **confusion matrix** is a table used to evaluate the performance of a classification model. It summarizes the results of predictions compared to actual outcomes in a matrix format:\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)    | False Negative (FN)   |\n",
    "| **Actual Negative** | False Positive (FP)   | True Negative (TN)    |\n",
    "\n",
    "It tells you how many correct and incorrect predictions your model made for each class, providing insight into errors like false positives and false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "- **Precision**: Measures the accuracy of positive predictions.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "  \\]\n",
    "\n",
    "  It answers: *Of the instances predicted as positive, how many were actually positive?*\n",
    "\n",
    "- **Recall (Sensitivity)**: Measures the model's ability to identify positive instances.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "  \\]\n",
    "\n",
    "  It answers: *Of the actual positives, how many did the model correctly predict as positive?*\n",
    "\n",
    "---\n",
    "\n",
    "### Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "To interpret errors in a confusion matrix:\n",
    "- **False Positives (FP)**: The model predicted a positive outcome when the actual outcome was negative. This indicates over-prediction of the positive class.\n",
    "- **False Negatives (FN)**: The model predicted a negative outcome when the actual outcome was positive. This indicates under-prediction of the positive class.\n",
    "\n",
    "By analyzing FP and FN rates, you can adjust the model to reduce the specific type of error that is more costly in your context.\n",
    "\n",
    "---\n",
    "\n",
    "### Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n",
    "\n",
    "Common metrics include:\n",
    "\n",
    "- **Accuracy**: The proportion of correctly predicted instances.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "  \\]\n",
    "\n",
    "- **Precision**: The proportion of predicted positives that are actually positive.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "  \\]\n",
    "\n",
    "- **Recall**: The proportion of actual positives that are correctly predicted.\n",
    "  \n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "  \\]\n",
    "\n",
    "- **F1-Score**: The harmonic mean of precision and recall, providing a balanced metric.\n",
    "  \n",
    "  \\[\n",
    "  \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "**Accuracy** is influenced by all four components of the confusion matrix: TP, TN, FP, and FN. A model can have high accuracy if the number of TNs is large, even if it performs poorly on predicting positives. Therefore, accuracy alone may not provide a full picture, especially in cases of **class imbalance**, where one class significantly outnumbers the other.\n",
    "\n",
    "---\n",
    "\n",
    "### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?\n",
    "\n",
    "A confusion matrix can help you identify biases by:\n",
    "- **Class Imbalance**: If your model consistently predicts the majority class (high FP or FN for minority class), it may be biased towards the majority class.\n",
    "- **Error Types**: If the model has many false positives (FP) or false negatives (FN), you can infer whether it favors one type of error over another, indicating potential bias.\n",
    "  \n",
    "By examining the distribution of errors, you can adjust the model (e.g., tuning thresholds) to mitigate such biases.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
