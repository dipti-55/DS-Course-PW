{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Assignment\n",
    "\n",
    "### Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "The **Euclidean distance** is the straight-line distance between two points, calculated as:\n",
    "\n",
    "\\[\n",
    "d(p, q) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}\n",
    "\\]\n",
    "\n",
    "The **Manhattan distance** is the sum of absolute differences between the points, calculated as:\n",
    "\n",
    "\\[\n",
    "d(p, q) = \\sum_{i=1}^{n} |p_i - q_i|\n",
    "\\]\n",
    "\n",
    "**Main differences**:\n",
    "- **Euclidean distance** measures straight-line distance, which works well when features are on a similar scale.\n",
    "- **Manhattan distance** is more suited for cases where the feature differences follow grid-like paths (e.g., city blocks).\n",
    "\n",
    "**Effect on KNN**:\n",
    "- Euclidean distance is sensitive to large feature differences due to squaring. It works well when features are continuous and scaled properly.\n",
    "- Manhattan distance may perform better when dealing with high-dimensional data or when features differ on a grid-like pattern.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "The optimal value of **k** can be determined using:\n",
    "\n",
    "1. **Cross-validation**: Split the dataset into training and validation sets. Train the model with different values of k and evaluate the performance using metrics such as accuracy (for classification) or mean squared error (for regression).\n",
    "   \n",
    "2. **Grid Search**: Perform an exhaustive search over a range of k values and select the one that yields the best performance.\n",
    "\n",
    "3. **Elbow method**: Plot the error rate as a function of k and select the k value where the error rate stabilizes (i.e., the \"elbow\" point).\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "\n",
    "The choice of **distance metric** directly impacts how neighbors are determined:\n",
    "\n",
    "- **Euclidean distance** is more appropriate when features are continuous and of similar scale.\n",
    "- **Manhattan distance** might be better when the data has high dimensions, or when features are measured in grid-like patterns.\n",
    "\n",
    "You might choose one over the other based on:\n",
    "- **Scale of data**: Euclidean is sensitive to feature scaling.\n",
    "- **Dimensionality**: Manhattan may handle high-dimensional data better.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "Common hyperparameters in KNN:\n",
    "- **k (number of neighbors)**: Controls the trade-off between bias and variance. A smaller k can lead to overfitting (low bias, high variance), while a larger k can underfit (high bias, low variance).\n",
    "- **Distance metric**: Affects how neighbors are selected. Popular choices are Euclidean and Manhattan distances.\n",
    "- **Weighting of neighbors**: Neighbors can be weighted by distance, with closer neighbors having more influence.\n",
    "\n",
    "**Tuning techniques**:\n",
    "- **Grid Search/Random Search**: Explore combinations of k, distance metrics, and weighting methods.\n",
    "- **Cross-validation**: Evaluate model performance across multiple folds to ensure generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "The size of the **training set** affects KNN as it directly influences the number of available neighbors. A larger training set:\n",
    "- **Improves accuracy**: More data points provide more accurate neighbors.\n",
    "- **Increases computational cost**: Larger datasets require more computation for distance calculations.\n",
    "\n",
    "**Optimization techniques**:\n",
    "- **Data sampling**: Use techniques like stratified sampling to reduce dataset size while preserving class distribution.\n",
    "- **Dimensionality reduction**: Apply PCA (Principal Component Analysis) or feature selection to reduce the number of features and simplify distance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "**Potential drawbacks**:\n",
    "- **Computational inefficiency**: KNN requires calculating distances for all data points, which can be slow for large datasets.\n",
    "- **Curse of dimensionality**: High-dimensional data can make distance metrics less informative, as all points tend to become equidistant.\n",
    "- **Sensitivity to noise**: KNN can be overly affected by noisy data points or outliers.\n",
    "\n",
    "**Solutions**:\n",
    "- **Dimensionality reduction**: Use PCA or feature selection to reduce the dimensionality of the data.\n",
    "- **Efficient algorithms**: Implement algorithms like KD-trees or Ball-trees to optimize neighbor search.\n",
    "- **Preprocessing**: Apply data normalization, outlier detection, and noise reduction techniques before applying KNN.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
