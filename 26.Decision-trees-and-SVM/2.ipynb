{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment Questions: Decision Tree-2\n",
    "\n",
    "### Q1. Import the dataset and examine the variables. Use descriptive statistics and visualizations to understand the distribution and relationships between the variables.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Descriptive statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation between Variables')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot to visualize the relationships\n",
    "sns.pairplot(data, hue='Outcome')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. Preprocess the data by cleaning missing values, removing outliers, and transforming categorical variables into dummy variables if necessary.\n",
    "\n",
    "```python\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Assuming missing values in 'SkinThickness' and 'Insulin'\n",
    "data['SkinThickness'].fillna(data['SkinThickness'].median(), inplace=True)\n",
    "data['Insulin'].fillna(data['Insulin'].median(), inplace=True)\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# No categorical variables, so no need to create dummy variables\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. Split the dataset into a training set and a test set. Use a random seed to ensure reproducibility.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. Use a decision tree algorithm, such as ID3 or C4.5, to train a decision tree model on the training set. Use cross-validation to optimize the hyperparameters and avoid overfitting.\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Set up hyperparameters for cross-validation\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. Evaluate the performance of the decision tree model on the test set using metrics such as accuracy, precision, recall, and F1 score. Use confusion matrices and ROC curves to visualize the results.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, best_dt.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q6. Interpret the decision tree by examining the splits, branches, and leaves. Identify the most important variables and their thresholds. Use domain knowledge and common sense to explain the patterns and trends.\n",
    "\n",
    "```python\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(best_dt, feature_names=X.columns, class_names=['Non-Diabetic', 'Diabetic'], filled=True)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "importances = best_dt.feature_importances_\n",
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "print(feature_importance.sort_values(by='Importance', ascending=False))\n",
    "\n",
    "# Interpretation:\n",
    "# Based on the tree and feature importance, glucose level is the most critical factor for predicting diabetes, followed by age and BMI. The model likely splits on glucose first, since it provides the most information gain.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Q7. Validate the decision tree model by applying it to new data or testing its robustness to changes in the dataset or the environment. Use sensitivity analysis and scenario testing to explore the uncertainty and risks.\n",
    "\n",
    "```python\n",
    "# Sensitivity analysis can be done by slightly modifying the test set or adding synthetic noise to see how the model reacts.\n",
    "import numpy as np\n",
    "\n",
    "# Introduce small noise to the test data\n",
    "X_test_noisy = X_test + np.random.normal(0, 0.01, X_test.shape)\n",
    "\n",
    "# Predict and evaluate on noisy data\n",
    "y_pred_noisy = best_dt.predict(X_test_noisy)\n",
    "accuracy_noisy = accuracy_score(y_test, y_pred_noisy)\n",
    "\n",
    "print(f\"Accuracy with noisy data: {accuracy_noisy}\")\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
