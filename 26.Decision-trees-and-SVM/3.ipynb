{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment Questions: Support Vector Machines-1\n",
    "\n",
    "### Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The mathematical formula for a linear SVM involves finding a hyperplane that maximizes the margin between two classes. The decision boundary is given by:\n",
    "\\[\n",
    "f(x) = w^T x + b\n",
    "\\]\n",
    "Where:\n",
    "- \\(w\\) is the weight vector.\n",
    "- \\(x\\) is the input feature vector.\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "The classification rule is:\n",
    "\\[\n",
    "y = \\text{sign}(w^T x + b)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective of a linear SVM is to minimize the following objective function (primal form):\n",
    "\\[\n",
    "\\min_{w,b} \\frac{1}{2} ||w||^2 \\quad \\text{subject to} \\quad y_i (w^T x_i + b) \\geq 1 \\quad \\forall i\n",
    "\\]\n",
    "This function aims to find the hyperplane that maximizes the margin between the two classes while ensuring correct classification of all training samples.\n",
    "\n",
    "In the soft-margin SVM (to allow for some misclassification), a regularization term with slack variables \\(\\xi_i\\) is added:\n",
    "\\[\n",
    "\\min_{w,b} \\frac{1}{2} ||w||^2 + C \\sum_i \\xi_i \\quad \\text{subject to} \\quad y_i (w^T x_i + b) \\geq 1 - \\xi_i \\quad \\forall i\n",
    "\\]\n",
    "Where \\(C\\) is the regularization parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The kernel trick allows SVM to classify non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space without explicitly computing the transformation. Common kernel functions include:\n",
    "- **Linear kernel**: \\( K(x_i, x_j) = x_i^T x_j \\)\n",
    "- **Polynomial kernel**: \\( K(x_i, x_j) = (x_i^T x_j + 1)^d \\)\n",
    "- **Radial Basis Function (RBF) kernel**: \\( K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2) \\)\n",
    "\n",
    "The kernel function computes the dot product of the transformed data points directly in the higher-dimensional space, enabling SVM to work with complex data patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary (or hyperplane). These points define the margin of the classifier and are critical in determining the decision boundary. The SVM model only depends on these support vectors, not on other points in the training set.\n",
    "\n",
    "For example, in a two-class problem, the support vectors are the points from each class that are closest to the hyperplane. The distance between these points and the hyperplane is maximized to ensure the widest possible margin.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a dataset\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Train a linear SVM\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot hyperplane, margins, and support vectors\n",
    "def plot_svm_boundary(clf, X, y):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "    ax = plt.gca()\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "    # Highlight support vectors\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "               linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "    plt.title('Hyperplane and Margins in SVM')\n",
    "    plt.show()\n",
    "\n",
    "plot_svm_boundary(clf, X, y)\n",
    "```\n",
    "\n",
    "- **Hyperplane**: The decision boundary that separates the two classes.\n",
    "- **Marginal Plane**: The boundaries that lie at a distance equal to the margin from the hyperplane, on either side.\n",
    "- **Hard Margin**: SVM with no tolerance for misclassification (only for linearly separable data).\n",
    "- **Soft Margin**: SVM with some tolerance for misclassification (used when data is not linearly separable).\n",
    "\n",
    "---\n",
    "\n",
    "### Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only two features for easy visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_clf = SVC(kernel='linear', C=1.0)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Plot the decision boundaries\n",
    "def plot_decision_boundaries(X, y, model):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.title('SVM Decision Boundaries')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundaries(X_test, y_test, svm_clf)\n",
    "\n",
    "# Try different values of the regularization parameter C\n",
    "for C in [0.1, 1, 10]:\n",
    "    svm_clf = SVC(kernel='linear', C=C)\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy with C={C}: {accuracy * 100:.2f}%')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
