{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Assignment Questions:\n",
    "\n",
    "#### Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the numerical features if necessary.\n",
    "\n",
    "**Answer:**\n",
    "- Handle missing values: Use appropriate methods like imputation (mean, median, mode) or removal of rows/columns with too many missing values.\n",
    "- Encode categorical variables: Use one-hot encoding or label encoding for categorical variables such as `sex`, `chest pain type`, etc.\n",
    "- Scale numerical features: Use standardization (Z-score normalization) or min-max scaling for features like `age`, `resting blood pressure`, `serum cholesterol`, and `maximum heart rate`.\n",
    "\n",
    "#### Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "\n",
    "**Answer:**\n",
    "- Use `train_test_split` from scikit-learn to divide the dataset, with 70% data for training and 30% for testing.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "#### Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each tree. Use the default values for other hyperparameters.\n",
    "\n",
    "**Answer:**\n",
    "- Use the `RandomForestClassifier` from scikit-learn to train the model with 100 trees and a maximum depth of 10.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "**Answer:**\n",
    "- Use metrics from scikit-learn to evaluate the model.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "```\n",
    "\n",
    "#### Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart disease risk. Visualize the feature importances using a bar chart.\n",
    "\n",
    "**Answer:**\n",
    "- Extract feature importances from the random forest model and visualize them using matplotlib.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feature_importances = clf.feature_importances_\n",
    "indices = np.argsort(feature_importances)[-5:]  # Get top 5 features\n",
    "\n",
    "plt.barh(range(len(indices)), feature_importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 5 Important Features')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try different values of the number of trees, maximum depth, minimum samples split, and minimum samples leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\n",
    "\n",
    "**Answer:**\n",
    "- Perform hyperparameter tuning using `GridSearchCV` or `RandomizedSearchCV` with 5-fold cross-validation.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Q7. Report the best set of hyperparameters found by the search and the corresponding performance metrics. Compare the performance of the tuned model with the default model.\n",
    "\n",
    "**Answer:**\n",
    "- Retrieve the best hyperparameters and compare the tuned model's performance with the default one.\n",
    "\n",
    "```python\n",
    "best_params = grid_search.best_params_\n",
    "tuned_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the tuned model\n",
    "tuned_pred = tuned_model.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, tuned_pred)\n",
    "```\n",
    "\n",
    "#### Q8. Interpret the model by analyzing the decision boundaries of the random forest classifier. Plot the decision boundaries on a scatter plot of two of the most important features. Discuss the insights and limitations of the model for predicting heart disease risk.\n",
    "\n",
    "**Answer:**\n",
    "- Use two of the most important features to plot the decision boundaries. Random forests are ensemble methods, so decision boundaries may not be as interpretable as single models like decision trees.\n",
    "\n",
    "```python\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Assume we are using two features X1 and X2\n",
    "X1, X2 = X_test[:, 0], X_test[:, 1]\n",
    "\n",
    "# Decision boundary plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_min, x_max = X1.min() - 1, X1.max() + 1\n",
    "y_min, y_max = X2.min() - 1, X2.max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=ListedColormap(('red', 'green')))\n",
    "plt.scatter(X1, X2, c=y_test, s=20, edgecolor='k')\n",
    "plt.title(\"Decision Boundaries of Random Forest Classifier\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Insights:\n",
    "- **Model Interpretation:** Random forests provide feature importance, but interpreting decision boundaries is challenging as they combine multiple decision trees.\n",
    "- **Limitations:** The model may have limited generalizability on unseen data and cannot easily capture relationships in small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
